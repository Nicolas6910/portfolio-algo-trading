#!/usr/bin/env python
# coding: utf-8
"""
BTC-USDT 1 min â€¢ LSTM bi-directionnel
OptimisÃ© : XLA, mixed-precision, timing des folds
Affiche un rapport dÃ©taillÃ© directement en console.
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ IMPORTS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from pathlib import Path
import numpy as np, pandas as pd, time, datetime as dt, platform, os, sys, talib, joblib
import tensorflow as tf
from tensorflow.keras.layers import Input, LSTM, Bidirectional, Dense, Dropout, BatchNormalization
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.losses import Huber
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import TimeSeriesSplit

# â€”â€”â€” AccÃ©lÃ©rations TensorFlow â€”â€”â€”
tf.keras.mixed_precision.set_global_policy("mixed_float16")
tf.config.optimizer.set_jit(True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ PARAMÃˆTRES â€”â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CSV       = Path(r"C:\Users\nicol\Desktop\Python API\BTC_Historical_Data\1m_BTC_Candles\BTC_2023\btc_usdt_february_2023.csv")
OUT       = Path(r"C:\Users\nicol\Desktop\models_btc_lstm_v3"); OUT.mkdir(parents=True, exist_ok=True)

LOOKBACK    = 25        # 25 minutes dâ€™historique
SPLITS      = 5         # validation croisÃ©e walk-forward
BATCH       = 300       # batch size
EPOCHS      = 30        # max epochs
PATIENCE    = 3         # patience early stopping
LR          = 1e-3      # learning rate
CLIP_NORM   = 1.0       # clip gradient

# â”€â”€â”€â”€â”€ 1. CHARGEMENT & FEATURES â”€â”€â”€â”€â”€
df = pd.read_csv(CSV, usecols=["timestamp","open","high","low","close","volume"])
df["timestamp"] = pd.to_datetime(df["timestamp"])
df.sort_values("timestamp", inplace=True)

# Features temporelles cycliques
df["hour"]   = df["timestamp"].dt.hour
df["minute"] = df["timestamp"].dt.minute
df["dow"]    = df["timestamp"].dt.dayofweek
df["hour_sin"]   = np.sin(2*np.pi*df["hour"]/24)
df["hour_cos"]   = np.cos(2*np.pi*df["hour"]/24)
df["minute_sin"] = np.sin(2*np.pi*df["minute"]/60)
df["minute_cos"] = np.cos(2*np.pi*df["minute"]/60)
df["dow_sin"]    = np.sin(2*np.pi*df["dow"]/7)
df["dow_cos"]    = np.cos(2*np.pi*df["dow"]/7)

# Lags
df["ret1"] = df["close"].pct_change()
df["ret5"] = df["close"].pct_change(5)

# Indicateurs TA-Lib
c, h, l, v = df["close"], df["high"], df["low"], df["volume"]
df["EMA_10"]  = talib.EMA(c, 10)
df["RSI_14"]  = talib.RSI(c, 14)
df["ATR_14"]  = talib.ATR(h, l, c, 14)

df.dropna(inplace=True)
df.reset_index(drop=True, inplace=True)

# CorrÃ©lation > 0.9 supprimÃ©e
feature_cols = [col for col in df.columns if col not in ("timestamp", "close")]
corr = df[feature_cols].corr().abs()
upper = corr.where(np.triu(np.ones_like(corr), k=1).astype(bool))
to_drop = [c for c in upper.columns if any(upper[c] > .9)]
df.drop(columns=to_drop, inplace=True)
feature_cols = [c for c in feature_cols if c not in to_drop]

# â€”â€”â€” 2. SCALING â€”â€”â€”
scaler_X = MinMaxScaler(); scaler_y = MinMaxScaler()
X_all = scaler_X.fit_transform(df[feature_cols]).astype(np.float32)
y_all = scaler_y.fit_transform(df[["close"]]).astype(np.float32).ravel()

def make_dataset(X, y, lookback):
    X_out, y_out = [], []
    for i in range(len(X)-lookback):
        X_out.append(X[i:i+lookback]); y_out.append(y[i+lookback])
    return np.asarray(X_out), np.asarray(y_out)

X, y = make_dataset(X_all, y_all, LOOKBACK)

# â€”â€”â€” 3. MODELE LSTM â€”â€”â€”
def build_model():
    model = Sequential([
        Input(shape=X.shape[1:]),
        Bidirectional(LSTM(64, recurrent_dropout=.2, return_sequences=True)),
        BatchNormalization(), Dropout(.3),
        Bidirectional(LSTM(64, recurrent_dropout=.2)),
        BatchNormalization(), Dropout(.3),
        Dense(64, activation="relu"),
        Dense(1, dtype="float32")
    ])
    model.compile(Adam(LR, clipnorm=CLIP_NORM), loss=Huber(), metrics=["mae"])
    return model

# â€”â€”â€” 4. ENTRAINEMENT WALK-FORWARD â€”â€”â€”
tscv = TimeSeriesSplit(n_splits=SPLITS)
metrics, durations, loss_histories, preview_preds = [], [], [], []
fold = 1

total_start = time.perf_counter()
print(f"\n===== EntraÃ®nement LSTM BTC 1min ({SPLITS} folds) =====")

for train_idx, val_idx in tscv.split(X):
    print(f"\nðŸ”¹ Fold {fold}/{SPLITS} â€” train {len(train_idx)} val {len(val_idx)}")
    model = build_model()
    cb = [
        EarlyStopping(patience=PATIENCE, restore_best_weights=True, monitor="val_loss"),
        ReduceLROnPlateau(patience=2, factor=.5, min_lr=1e-5, monitor="val_loss")
    ]
    t0 = time.perf_counter()
    hist = model.fit(
        X[train_idx], y[train_idx],
        validation_data=(X[val_idx], y[val_idx]),
        epochs=EPOCHS,
        batch_size=BATCH,
        shuffle=False,
        callbacks=cb,
        verbose=2
    )
    fold_time = time.perf_counter() - t0
    durations.append(fold_time)

    # KPIs
    preds = scaler_y.inverse_transform(model.predict(X[val_idx])).ravel()
    reals = scaler_y.inverse_transform(y[val_idx, None]).ravel()
    mae  = np.mean(np.abs(preds - reals))
    rmse = np.sqrt(np.mean((preds - reals)**2))
    acc  = (np.sign(np.diff(preds)) == np.sign(np.diff(reals))).mean()
    metrics.append((mae, rmse, acc))
    print(f"â±ï¸  {fold_time:.1f}s  |  MAE:{mae:.2f}  RMSE:{rmse:.2f}  DirAcc:{acc:.2%}")
    # PrÃ©visualisation : premiers et derniers exemples
    preview_preds.append({
        "preds": preds[:3].round(2).tolist() + ["..."] + preds[-3:].round(2).tolist(),
        "reals": reals[:3].round(2).tolist() + ["..."] + reals[-3:].round(2).tolist()
    })
    fold += 1

total_time = time.perf_counter() - total_start

# â€”â€”â€” 5. EXPORT SCALERS & KPI (optionnel) â€”â€”â€”
joblib.dump(scaler_X, OUT / "scaler_X.save")
joblib.dump(scaler_y, OUT / "scaler_y.save")
np.save(OUT / "cv_metrics.npy", np.array(metrics))
np.save(OUT / "fold_times.npy", np.array(durations))

# â€”â€”â€” 6. RAPPORT CONSOLE â€”â€”â€”
print("\n========== RAPPORT DÃ‰TAILLÃ‰ ==========\n")
print(f"Date               : {dt.datetime.now().strftime('%Y-%m-%d %H:%M')}")
print(f"Total duration     : {total_time/60:.1f} min")
print(f"Dataset shape      : {df.shape}")
print(f"Lookback window    : {LOOKBACK} min ({LOOKBACK} pas)")
print(f"Features retenues  : {feature_cols}")
print(f"Batch size         : {BATCH}")
print(f"Splits (folds)     : {SPLITS}")
print(f"Epochs (max)       : {EPOCHS}")
print(f"Patience           : {PATIENCE}")
print(f"Learning rate      : {LR}")
print(f"Clipnorm           : {CLIP_NORM}")
print("\nâ€”â€” KPIs par fold â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”")
print("Fold |    MAE     |   RMSE    | Dir. Acc |   DurÃ©e")
print("-" * 49)
for i, (mae, rmse, acc) in enumerate(metrics):
    print(f"{i+1:>4} | {mae:9.2f} | {rmse:8.2f} | {acc:8.2%} | {durations[i]/60:6.1f} min")
print("-" * 49)
means = np.mean(metrics, axis=0)
print(f" Moy | {means[0]:9.2f} | {means[1]:8.2f} | {means[2]:8.2%} | {np.mean(durations)/60:6.1f} min")
print("\nâ€”â€” Exemples de prÃ©dictions par fold â€”â€”â€”â€”")
for i, ex in enumerate(preview_preds, 1):
    print(f"Fold {i}: Preds: {ex['preds']} | Reals: {ex['reals']}")
print("\nâœ…  EntraÃ®nement terminÃ© !")
